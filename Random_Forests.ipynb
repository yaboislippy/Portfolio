{"cells":[{"cell_type":"markdown","source":["# Random Forests"],"metadata":{"id":"IRPrKgqfaZJd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN9VMGrB7f2S"},"outputs":[],"source":["# Import libraries\n","import pandas as pd\n","\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["# Create toy data\n","X, y = make_classification(n_samples=1000, n_features=10,\n","                           n_informative=5, n_redundant=0,\n","                           random_state=123, shuffle=False)\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"],"metadata":{"id":"RmtckLTyjJga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code above generates synthetic data using the `make_classification` function, which creates a dataset with 1,000 samples and 10 features, of which 5 are informative. The `random_state` parameter ensures reproducibility, while `shuffle=False` maintains the order of the generated data. The `train_test_split` function then divides the data into training (70%) and testing (30%) sets, allowing for effective model training and evaluation. This approach provides a quick way to create a classification dataset and prepare it for machine learning tasks."],"metadata":{"id":"J6hVSLAAjLiM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmYXIFFc7f2Y","outputId":"becff4ce-fc8d-4a89-d0c2-aa48639aba1d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727760818521,"user_tz":-120,"elapsed":1280,"user":{"displayName":"Alfred Ndlovu","userId":"18188314822513300687"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy base: 0.9166666666666666\n","Accuracy ensemble: 0.9566666666666667\n"]}],"source":["# Create base model\n","base = DecisionTreeClassifier(max_depth=5)\n","\n","# Create an ensemble model\n","ensemble = BaggingClassifier(estimator=base, n_estimators=100, random_state=7)\n","\n","# Fit the base model on the training data\n","base.fit(X_train,y_train)\n","\n","# Fit the ensemble model on the training data\n","ensemble.fit(X_train,y_train)\n","\n","print(\"Accuracy base:\",base.score(X_test, y_test))\n","print(\"Accuracy ensemble:\",ensemble.score(X_test, y_test))"]},{"cell_type":"markdown","source":["In this code, a Decision Tree classifier is created as the base model, with a maximum depth of 5 to prevent overfitting. An ensemble model is then constructed using the `BaggingClassifier`, which combines 100 instances of the base decision tree to improve the model's accuracy and stability. The `random_state` parameter ensures that the results can be reproduced. After initialising the models, the code fits both the base and ensemble models to the training data, enabling them to learn from it. Finally, the accuracy of both models is evaluated using the test data and printed to compare their performance. This process highlights the potential benefits of using ensemble methods, such as bagging, which can often outperform single classifiers by reducing variance and improving predictions."],"metadata":{"id":"p8mkGLqlkAvl"}},{"cell_type":"markdown","metadata":{"id":"xV_ujLdW7f2b"},"source":["###  Feature importance scores\n","\n","A property of the Random Forest ensemble method in sklearn is that it lets you print importance scores for features in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMD73jvh7f2c","outputId":"56cab45b-9f3c-4b7d-d4d1-22a96682747a","colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"status":"ok","timestamp":1727760824526,"user_tz":-120,"elapsed":648,"user":{"displayName":"Alfred Ndlovu","userId":"18188314822513300687"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.367378\n","1    0.240483\n","2    0.117017\n","3    0.108648\n","4    0.064458\n","7    0.023878\n","8    0.022136\n","9    0.020229\n","6    0.019116\n","5    0.016657\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.367378</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.240483</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.117017</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.108648</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.064458</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.023878</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.022136</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.020229</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.019116</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.016657</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":6}],"source":["forest = RandomForestClassifier(n_estimators=100, random_state=7)\n","forest.fit(X_train, y_train)\n","\n","feature_imp = pd.Series(forest.feature_importances_).sort_values(ascending=False)\n","feature_imp"]},{"cell_type":"markdown","metadata":{"id":"DicM0dwn7f2d"},"source":["At the start of this notebook, we specified that this dataset has 10 features, of which 5 are informative. The classifier relied on the bottom five features (0,1,2,3,4) less than the higher numbered features (7,8,9,6,5). An advantage of investigating the importance of features is that irrelevant features can be removed. This removal of noise tends to improve performance and reduce training time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YXzZrgj7f2e","outputId":"1b64d61c-79e3-4a31-c268-a41cd181ad6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727760872702,"user_tz":-120,"elapsed":736,"user":{"displayName":"Alfred Ndlovu","userId":"18188314822513300687"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy base: 0.91\n","Accuracy ensemble: 0.9366666666666666\n"]}],"source":["# select important features\n","X = X[:, :5]\n","\n","# retrain\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n","base.fit(X_train,y_train)\n","ensemble.fit(X_train,y_train)\n","\n","print(\"Accuracy base:\",base.score(X_test, y_test))\n","print(\"Accuracy ensemble:\",ensemble.score(X_test, y_test))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}